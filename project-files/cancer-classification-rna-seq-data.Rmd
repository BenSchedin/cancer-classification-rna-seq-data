---
title: "Cancer Classification With RNA-Seq Data"
author: "B. D. Schedin"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
    toc_depth: 3
    theme: yeti
    highlight: pygments
    fig_caption: yes
fontsize: 11pt
---

try different cost and gamma parameters for svm tune  
try linear and radial svm kernels  
add k-fold cv to nn implementation  
remove ci from knn 1:100 visual  
add accuracy assessment to hclust  
update hclust plots  
replace source files with github source compared to local source  
double check and add model assumption analysis to lda/qda  
how many components to select for? perform classification with varying number of PCs to test accuracy increase?

```{r setup, include=FALSE}

# Global chunk options
knitr::opts_chunk$set(echo = TRUE)

# Project paths
projRoot <- "C:/_git_/cancer-classification-rna-seq-data/"

# Project libraries
library(tidyverse)
library(e1071)
library(class)
library(ggdendro)
library(neuralnet)
library(ggcorrplot)
library(MASS)
library(randomForest)

# Sourced files
source(paste0(projRoot, "r-code/functions/neural-net.R"))
source(paste0(projRoot, "r-code/functions/diag-accuracy.R"))

```

# I. ABSTRACT



# II. INTRODUCTION



# III. METHODOLOGY



# IV. RESULTS



# V. CONCLUSION



# VI. APPENDIX

## Preprocessing
The initial step in this analysis was to load the data into R and prepare it for use during the rest of the analysis. This preprocessing step included cleaning up unneeded columns, checking the data for missing values, removing genes which appear to be inactive (defined as genes with $expression = 0$ for $\geq 25\%$ of the available samples), and standardizing the features with the `scale` function. The scaling would prove to be unnecessary, as PCA was applied to the data for dimension reduction which includes scaling; however, the data was still scaled in case ad-hoc processing of the original data was desired.
```{r preprocessing, eval=FALSE}

# Loading raw data
rawData <- read.csv(paste0(projRoot, "data/raw/data.csv"))
rawLabels <- read.csv(paste0(projRoot, "data/raw/labels.csv"))

# Setting sample IDs to row names, removing sample ID columns
data <- rawData
rownames(data) <- rawData$X
data <- data %>% dplyr::select(-X)

labels <- rawLabels
rownames(labels) <- rawLabels$X
labels <- labels %>% dplyr::select(-X)

# Checking for missing values
missing <- sapply(data, function(x) sum(is.na(x)))
cat("Found", length(missing[missing > 0]), "columns with missing values.\n")

# Removing genes which are not expressed for more than 25% of samples
preCounts <- map_int(data, function(x) sum(x == 0))
data <- data[colMeans(data == 0) <= 0.25]
postCounts <- map_int(data, function(x) sum(x == 0))

cat("Removed", length(preCounts) - length(postCounts), "columns containing genes unexpressed in more than 25% of samples.\n")

# Scaling features
data <- scale(data)

# Exporting preprocessed data to file
save("data", "labels", file=paste0(projRoot, "data/processed/rna-seq-preprocessed"))

```
In total: No columns were found to have missing values, and 3,790 genes whose expressions were 0 in more than 25% of samples were removed from the data set. The resulting preprocessed data set contained 801 observations of 16,741 variables in the `data` object, and 801 observations of matching labels found in the `labels` object. This data was then saved to disk using the built-in `save` function for later use and quicker load times on subsequent loads.


## Principal Component Analysis
Following preprocessing, PCA was applied to the data using the `stats::prcomp` method. This was performed for two reasons. First and foremost, PCA was applied to reduce the dimensionality of the data. With the original data consisting of 20,532 columns (and the preprocessed data consisting of 16,741 columns), the cost of running a multitude of machine learning algorithms against the data would have been expensive. Second, PCA was applied to the data to reduce noise. Given the large number of columns, it was impossible to conclude that all genes expressed in ways that yielded equal predictive power. Performing PCA allowed for dimension reduction with the added benefit of selecting for linear combinations of genes which expressed the highest variance.
```{r pca}

# Setting chunk seed
set.seed(123)

# Loading preprocessed data
load(paste0(projRoot, "data/processed/rna-seq-preprocessed"))
data <- as.data.frame(data)

# PCA on all features
pca <- prcomp(data)

# Calculating cumulative variance for each principal component
cumVar <- cumsum((nrow(data) - 1) * ((pca$sdev)^2)/sum(data^2))
cumVarData <- data.frame(PC = 1:length(cumVar), CumulativeVariance = cumVar)

# Plotting the cumulative variance
ggplot(cumVarData, aes(x=PC, y=CumulativeVariance)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(title="Component Cumulative Variance", x="PC", y="Variance")
ggsave(paste0(projRoot, "figures/pca-cumulative-variance.png"), device="png", width=6, height=4)

# Visualizing the first two component scores
scores <- as.data.frame(pca$x[,1:2])
scores$Class <- labels$Class

ggplot(scores, aes(PC1, PC2, color=Class, shape=Class)) +
  geom_point() +
  geom_hline(yintercept=0, linetype="dotted") +
  geom_vline(xintercept=0, linetype="dotted") +
  theme_bw() +
  labs(title="Scores: PC1 and PC2", x="PC1 Scores", y="PC2 Scores")
ggsave(paste0(projRoot, "figures/pca-component-scores-plot.png"), device="png", width=6, height=4)

# Determining the number of components needed to reach 50% cumulative variance
cat("Number of components required to reach 50% cumulative variance:", length(cumVar[cumVar <= 0.50]), "\n")

# Extracting the first 8 principal components
pca8 <- as.data.frame(pca$x[, 1:8])

# Combining selected components and labels to create a "full" data set
full <- pca8
full$Class <- as.factor(labels$Class)

```
After performing principal component analysis, a plot was created which visualized the cumulative variance of each component produced. From this plot it is clear that the first few components account for a relatively large proportion of the total variance. With that in mind, a cumulative variance threshold of $50\%$ was chosen to select an initial number of components to work with. This cutoff was somewhat arbitrary, but ultimately ended up yielding very high ($>95\%$) classification rates across classifiers, and also greatly reduced the number of columns that needed to be processed.

A plot of the scores from PC1 and PC2 was also created to see if any groupings were obvious in the components. From the plot scores plot, `COAD`, `KIRC`, and `BRCA` form fairly distinct clusters. Conversely, `LUAD` and `PRAD` samples exhibit significant overlap between samples. Overall, the most separable classes are `KIRC`, `COAD`, `BRCA`, `LUAD`, and `PRAD` in descending order. TODO: quantify separability/cluster signatures?

## k-Means Clustering
Before performing traditional classifications, clustering was applied to the data to see how accurately natural groupings could be formed. The first clustering algorithm applied was k-Means Clustering. The accuracy of these groupings was assessed by choosing the dominant in-cluster class as the sample label. All other samples assigned to each cluster was considered to be an error. The clustering results can be seen in the following plot (with the cluster assignment resulting from the algorithm represented as shapes and the true class label represented by color), as well as in the classification results table.

TODO: add methods for choosing 'k'
```{r kmeans}

# Setting chunk seed
set.seed(123)

# Applying k-means to the data
k <- 5
kmeans <- kmeans(pca8, centers=k, iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))

# Visualizing the results
kmeansResults <- data.frame(pca8)
kmeansResults$Class <- as.factor(labels$Class)
kmeansResults$Cluster <- as.factor(kmeans$cluster)

ggplot(kmeansResults, aes(PC1, PC2)) +
  geom_point(aes(color=Class, shape=Cluster)) +
  theme_bw() +
  labs(title=paste0(k, "-Means Clustering", sep=""), x="PC1", y="PC2")
ggsave(paste0(projRoot, "figures/pca-kmeans.png"), device="png", width=6, height=4)

# Displaying the results
predTable <- table(kmeansResults$Class, kmeansResults$Cluster)
predTable

# Exporting results table to .csv file
predTable <- as.data.frame.matrix(predTable)
write.csv(predTable, file=paste0(projRoot, "tables/kmeans.csv"))

# Calculating "error" rate
dominant <- sapply(predTable, function(x) max(x))
total <- sapply(predTable, function(x) sum(x))

cat("Accuracy (based on dominant class samples per cluster): ", round(sum(dominant) / sum(total), 4) * 100, "% \n", sep="")

```
With $k=5$, the k-Means algorithm was able to successfully group 91.01% of the samples indicating that the sample classes are fairly distinct. The cluster plot shows this visually, with most sample classes (colors) matching the assigned clusters (shapes).

## Hierarchical Clustering
TODO: add hclust intro
```{r hclust}

# Setting chunk seed
set.seed(123)

# Applying hierarchical clustering with multiple linkages
hcAverage <- hclust(dist(pca8), method="average")
hcSingle <- hclust(dist(pca8), method="single")
hcComplete <- hclust(dist(pca8), method="complete")

# Visualizing results with dendrograms
ggdendrogram(hcAverage, rotate=FALSE, size=2)
ggdendrogram(hcSingle, rotate=FALSE, size=2)
ggdendrogram(hcComplete, rotate=FALSE, size=2)

```
TODO: add hclust results

## k-Nearest Neighbors
The first supervised classification method used in this analysis was k-Nearest Neighbors (kNN). TODO: explain why this method was used. For this application of kNN, 75% of the samples were used to train the model with the remaining 25% being used for testing. In order to determine the best value for `k`, 100 models were created for $1 \le k \le 100$ and the model with the highest accuracy was used to report the final results. A visualization of the model accuracies for $1 \le k \le 100$ was also created and can be seen below. Note: Smoothing was applied to the visual, with 95% confidence intervals for the true value shown in gray.
```{r knn, warning=FALSE}

# Setting the seed for the chunk
set.seed(123)

# Splitting the data into training and test sets, 75-25 split
trainIndices <- sample(1:nrow(pca8), floor(0.75 * nrow(pca8)), replace=FALSE)
testIndices <- (1:nrow(pca8))[-trainIndices]

trainData <- as.data.frame(pca8[trainIndices, ])
trainLabels <- as.data.frame(labels[trainIndices, ])
colnames(trainLabels) <- "Class"

testData <- as.data.frame(pca8[testIndices, ])
testLabels <- as.data.frame(labels[testIndices, ])
colnames(testLabels) <- "Class"

# Implementing KNN for k=1:100
knnResults <- list()
knnAccuracies <- list()

for (i in 1:100)
{
  knnTemp <- knn(trainData, testData, cl=trainLabels$Class, k=i)
  predTableTemp <- table(knnTemp, testLabels[[1]])
  accuracy <- round(sum(diag(predTableTemp)) / nrow(testData), 4) * 100
  knnResults[[i]] <- predTableTemp
  knnAccuracies[[i]] <- accuracy
}

# Displaying best model results for k=1:100
knnAccuracies <- unlist(knnAccuracies)
bestAccuracy <- max(knnAccuracies)
bestIndex <- which(knnAccuracies == max(knnAccuracies))[1]
predTable <- knnResults[[bestIndex]]

cat("Best model accuracy was ", bestAccuracy, "% with k=", bestIndex, "\n", sep="")
predTable

# Plotting results for k=1:100
ggplot(data=NULL, aes(x=1:100, y=knnAccuracies)) +
  geom_smooth() +
  theme_bw() +
  labs(title="K-Nearest Neighbors, k=1:100", x="k", y="Accuracy")
ggsave(paste0(projRoot, "figures/knn-accuracies.png"), device="png", width=6, height=4)

# Exporting results table to .csv file
predTable <- as.data.frame.matrix(predTable)
write.csv(predTable, file=paste0(projRoot, "tables/knn.csv"))

```

Out of the 100 models created, the model with $k=2$ performed the best with a test accuracy of 98.51%. Only 3 samples were classified incorrectly with 1 `KIRC` sample being classified as `BRCA`, 1 `LUAD` sample being classified as `BRCA`, and 1 `BRCA` sample being classified as `LUAD`. The results for each model for $1 \le k \le 100$ also showed a decrease in test accuracy as the local neighborhood size (`k`) increased, falling from $\approx 98 \%$ at $k=1$ to $\approx 96 \%$ at $k=100$. This indicates that cluster groupings, while relatively homogeneous, are close to other clusters in terms of euclidean distance. Furthermore, the most frequently misclassified class (`BRCA`) demonstrated with most overlap with other classes in both PCA and k-Means results.

## Support Vector Machines
Support Vector Machines (SVM) was applied to the data with both linear and radial kernels using 75% of the samples for training and the remaining 25% for testing. In order to tune the hyperparameters for the models, `e1071::tune` was used with $cost = \{0.01, 0.1, 1, 5, 10, 50\}$ and $\gamma=\{0.5, 1, 2, 3, 4\}$ for both kernels.
```{r svm, warning=FALSE}

# Setting chunk seed
set.seed(123)

# Splitting data into training and test sets, 75-25 split
trainIndices <- sample(1:nrow(full), 0.75*nrow(full), replace=FALSE)

trainData <- full[trainIndices,]
testData <- full[-trainIndices,]

# Training model and tuning hyperparameters with e1071::tune with linear kernel
tuneLinearSVM <- tune(svm,
                      train.x=trainData[, 1:8],
                      train.y=trainData$Class,
                      kernel="linear",
                      type="C",
                      ranges=list(cost=c(0.01, 0.1, 1, 5, 10, 50),
                                  gamma=c(0.5, 1, 2, 3, 4)))
summary(tuneLinearSVM)

# Extracting best resulting model
bestLinearSVM <- tuneLinearSVM$best.model
summary(bestLinearSVM)

# Testing best model and displaying results
yPred <- predict(bestLinearSVM, testData[,1:8])

predTable <- table(predicted=yPred, truth=testData$Class)
predTable

# Calculating error rate
cat("Linear SVM Accuracy: ", round(sum(diag(predTable)) / nrow(testData), 4) * 100, "%\n", sep="")

# Exporting results table to .csv file
predTable <- as.data.frame.matrix(predTable)
write.csv(predTable, file=paste0(projRoot, "tables/linear-svm.csv"))



# Training model and tuning hyperparameters with e1071::tune with radial kernel
tuneRadialSVM <- tune(svm,
                      train.x=trainData[, 1:8],
                      train.y=trainData$Class,
                      kernel="linear",
                      type="C",
                      ranges=list(cost=c(0.01, 0.1, 1, 5, 10, 50),
                                  gamma=c(0.5, 1, 2, 3, 4)))
summary(tuneRadialSVM)

# Extracting best resulting model
bestRadialSVM <- tuneRadialSVM$best.model
summary(bestRadialSVM)

# Testing best model and displaying results
yPred <- predict(bestRadialSVM, testData[,1:8])

predTable <- table(predicted=yPred, truth=testData$Class)
predTable

# Calculating error rate
cat("Radial SVM Accuracy: ", round(sum(diag(predTable)) / nrow(testData), 4) * 100, "%\n", sep="")

# Exporting results table to .csv file
predTable <- as.data.frame.matrix(predTable)
write.csv(predTable, file=paste0(projRoot, "tables/radial-svm.csv"))

```

The best resulting linear model from 10-fold cross validation utilized $cost = 10$ and $\gamma=0.5$, had 42 support vectors, and produced a test accuracy of 98.51%. For the radial kernel, the model utilized $cost = 0.1$ and $\gamma = 0.5$, had 94 support vectors, and produced a test accuracy of 99%. This indicated that the data is not perfectly linearly separable, and benefited from projection prior to classification. 

## Linear Discriminant Analysis
```{r lda}

# Setting the chunk seed
set.seed(123)

# Splitting data into training and test sets with 75-25 split
trainIndices <- sample(1:nrow(full), floor(0.75*nrow(full)), replace=FALSE)

trainData <- full[trainIndices, ]
testData <- full[-trainIndices, ]

# Building QDA model
ldaTrain <- lda(Class ~ ., data=trainData)
ldaTest <- predict(ldaTrain, newdata=testData)

# Reporting test accuracy
testPred <- table(testData$Class, ldaTest$class)
testPred

cat("Test model accuracy: ", accuracyDiag(testPred), "%\n", sep="")

# Exporting test results to .csv file
testPred <- as.data.frame.matrix(testPred)
write.csv(testPred, file=paste0(projRoot, "tables/lda-test.csv"))

```

## Quadratic Discriminant Analysis
```{r qda}

# Setting chunk seed
set.seed(123)

# Checking correlations between components
compCor <- cor(pca8)

ggcorrplot(compCor) + 
  theme(axis.text.x = element_text(size=11, angle=90, hjust=0.99, vjust=0.3), axis.text.y=element_text(size=11)) + 
  labs(title = "Component Correlations", x="Variable 1", y="Variable 2")
ggsave(paste0(projRoot, "figures/comp-correlations.png"), device="png", width=6, height=4)

# Checking for GMM violations
gmmData <- reshape2::melt(full, id="Class")

ggplot(gmmData, aes(x=value, fill=Class)) +
  geom_density(alpha=0.25) +
  theme_bw() +
  labs(title="Class Distributions", x="Value", y="Density")
ggsave(paste0(projRoot, "figures/comp-class-distributions.png"), device="png", width=6, height=4)

# Splitting data into training and test sets with 75-25 split
trainIndices <- sample(1:nrow(full), floor(0.75*nrow(full)), replace=FALSE)

trainData <- full[trainIndices, ]
testData <- full[-trainIndices, ]

# Building QDA model
qdaTrain <- qda(Class ~ ., data=trainData)
qdaTest <- predict(qdaTrain, newdata=testData)

# Reporting test accuracy
testPred <- table(testData$Class, qdaTest$class)
testPred

cat("Test model accuracy: ", accuracyDiag(testPred), "%\n", sep="")

# Exporting test results to .csv file
testPred <- as.data.frame.matrix(testPred)
write.csv(testPred, file=paste0(projRoot, "tables/qda-test.csv"))

```

## Neural Network
```{r nn}

# Setting chunk seed
set.seed(123)

# Splitting data into train and test sets, 75-25 split
trainIndices <- sample(1:nrow(full), nrow(full)*0.75, replace=FALSE)

trainData <- full[trainIndices,]
testData <- full[-trainIndices,]

# Fitting the model
nnet <- neuralnet(Class ~ ., trainData, hidden=c(10, 12), act.fct="logistic", linear.output=FALSE)

# Displaying the model
plot(nnet, show.weights=FALSE, information=TRUE, intercept=FALSE, rep="best", col.hidden="blue")

# Checking train set accuracy
trainPred <- nnPred(trainData, nnet)
trainPred

cat("Training model accuracy: ", accuracyDiag(trainPred), "%\n", sep="")

# Exporting train results table to .csv file
trainPred <- as.data.frame.matrix(trainPred)
write.csv(trainPred, file=paste0(projRoot, "tables/neural-net-train.csv"))

# Checking test set accuracy
testPred <- nnPred(testData, nnet)
testPred

cat("Test model accuracy: ", accuracyDiag(testPred), "%\n", sep="")

# Exporting test results table to .csv file
testPred <- as.data.frame.matrix(testPred)
write.csv(testPred, file=paste0(projRoot, "tables/neural-net-test.csv"))

```

## Random Forest
```{r randforest}

# Setting chunk seed
set.seed(123)

# Splitting the data into training and test sets, 75-25 split
trainIndices <- sample(1:nrow(full), 0.75*nrow(full), replace=FALSE)

trainData <- full[trainIndices, ]
testData <- full[-trainIndices, ]

# Building the model
rf <- randomForest(Class ~ ., data=trainData, ntree=100)
rfPred <- predict(rf, testData)

# Reporting test accuracy
testPred <- table(predicted=rfPred, truth=testData$Class)
testPred

cat("Test model accuracy: ", accuracyDiag(testPred), "%\n", sep="")

# Exporting classification table to file
testPred <- as.data.frame.matrix(testPred)
write.csv(testPred, file=paste0(projRoot, "tables/random-forest-results.csv"))

```





# VII. REFERENCES







