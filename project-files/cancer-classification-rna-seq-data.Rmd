---
title: "Cancer Classification With RNA-Seq Data"
author: "B. D. Schedin"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
    toc_depth: 3
    theme: yeti
    highlight: pygments
    fig_caption: yes
fontsize: 11pt
---

```{r setup, include=FALSE}

# Global chunk options
knitr::opts_chunk$set(echo = TRUE)

# Project paths
projRoot <- "C:/_git_/cancer-classification-rna-seq-data/"

# Project libraries
library(tidyverse)
library(e1071)
library(class)
library(ggdendro)
library(neuralnet)

```

# 1. Abstract





# 2. Introduction





# 3. Methods and Results





# 4. Criticisms of Methodology
try different cost and gamma parameters for svm tune
try linear and radial svm kernels
add k-fold cv to nn implementation
remove ci from knn 1:100 visual
add accuracy assessment to hclust
update hclust plots




# 5. Conclusion





# 6. Appendix

## Dataset Information

## Preprocessing
```{r preprocessing, eval=FALSE}

# Loading raw data
rawData <- read.csv(paste0(projRoot, "data/raw/data.csv"))
rawLabels <- read.csv(paste0(projRoot, "data/raw/labels.csv"))

# Setting sample IDs to row names, removing sample ID columns
data <- rawData
rownames(data) <- rawData$X
data <- data %>% select(-X)

labels <- rawLabels
rownames(labels) <- rawLabels$X
labels <- labels %>% select(-X)

# Checking for missing values
missing <- sapply(data, function(x) sum(is.na(x)))
cat("Found", length(missing[missing > 0]), "columns with missing values.\n")

# Removing genes which are not expressed for more than 25% of samples
preCounts <- map_int(data, function(x) sum(x == 0))
data <- data[colMeans(data == 0) <= 0.25]
postCounts <- map_int(data, function(x) sum(x == 0))

cat("Removed", length(preCounts) - length(postCounts), "columns containing genes unexpressed in more than 25% of samples.\n")

# Scaling features
data <- scale(data)

# Exporting preprocessed data to file
save("data", "labels", file=paste0(projRoot, "data/processed/rna-seq-preprocessed"))

```

## Principal Component Analysis
```{r pca}

# Setting chunk seed
set.seed(123)

# Loading preprocessed data
load(paste0(projRoot, "data/processed/rna-seq-preprocessed"))
data <- as.data.frame(data)

# PCA on all features
pca <- prcomp(data)

# Calculating cumulative variance for each principal component
cumVar <- cumsum((nrow(data) - 1) * ((pca$sdev)^2)/sum(data^2))
cumVarData <- data.frame(PC = 1:length(cumVar), CumulativeVariance = cumVar)

# Plotting the cumulative variance
ggplot(cumVarData, aes(x=PC, y=CumulativeVariance)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(title="Component Cumulative Variance", x="PC", y="Variance")
ggsave(paste0(projRoot, "figures/pca-cumulative-variance.png"), device="png", width=6, height=4)

# Visualizing the first two component scores
scores <- as.data.frame(pca$x[,1:2])
scores$Class <- labels$Class

ggplot(scores, aes(PC1, PC2, color=Class, shape=Class)) +
  geom_point() +
  geom_hline(yintercept=0, linetype="dotted") +
  geom_vline(xintercept=0, linetype="dotted") +
  theme_bw() +
  labs(title="Scores: PC1 and PC2", x="PC1 Scores", y="PC2 Scores")
ggsave(paste0(projRoot, "figures/pca-component-scores-plot.png"), device="png", width=6, height=4)

# Determining the number of components needed to reach 50% cumulative variance
cat("Number of components required to reach 50% cumulative variance:", length(cumVar[cumVar <= 0.50]), "\n")

# Extracting the first 8 principal components
pca8 <- as.data.frame(pca$x[, 1:8])

```

## K-Means Clustering 
```{r kmeans}

# Setting chunk seed
set.seed(123)

# Applying k-means to the data
k <- 5
kmeans <- kmeans(pca8, centers=k, iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))

# Visualizing the results
kmeansResults <- data.frame(pca8)
kmeansResults$Class <- as.factor(labels$Class)
kmeansResults$Cluster <- as.factor(kmeans$cluster)

ggplot(kmeansResults, aes(PC1, PC2)) +
  geom_point(aes(color=Class, shape=Cluster)) +
  theme_bw() +
  labs(title=paste0(k, "-Means Clustering", sep=""), x="PC1", y="PC2")
ggsave(paste0(projRoot, "figures/pca-kmeans.png"), device="png", width=6, height=4)

# Displaying the results
predTable <- table(kmeansResults$Class, kmeansResults$Cluster)
predTable

# Exporting results table to .csv file
predTable <- as.data.frame.matrix(predTable)
write.csv(predTable, file=paste0(projRoot, "tables/kmeans.csv"))

# Calculating "error" rate
dominant <- sapply(predTable, function(x) max(x))
total <- sapply(predTable, function(x) sum(x))

cat("Accuracy (based on dominant class samples per cluster): ", round(sum(dominant) / sum(total), 4) * 100, "% \n", sep="")

```

## Hierarchical Clustering
```{r hclust}

# Setting chunk seed
set.seed(123)

# Applying hierarchical clustering with multiple linkages
hcAverage <- hclust(dist(pca8), method="average")
hcSingle <- hclust(dist(pca8), method="single")
hcComplete <- hclust(dist(pca8), method="complete")

# Visualizing results with dendrograms
ggdendrogram(hcAverage, rotate=FALSE, size=2)
ggdendrogram(hcSingle, rotate=FALSE, size=2)
ggdendrogram(hcComplete, rotate=FALSE, size=2)

```

## K-Nearest Neighbors
```{r knn, warning=FALSE}

# Setting the seed for the chunk
set.seed(123)

# Splitting the data into training and test sets, 75-25 split
trainIndices <- sample(1:nrow(pca8), floor(0.75 * nrow(pca8)), replace=FALSE)
testIndices <- (1:nrow(pca8))[-trainIndices]

trainData <- as.data.frame(pca8[trainIndices, ])
trainLabels <- as.data.frame(labels[trainIndices, ])
colnames(trainLabels) <- "Class"

testData <- as.data.frame(pca8[testIndices, ])
testLabels <- as.data.frame(labels[testIndices, ])
colnames(testLabels) <- "Class"

# Implementing KNN for k=1:100
knnResults <- list()
knnAccuracies <- list()

for (i in 1:100)
{
  knnTemp <- knn(trainData, testData, cl=trainLabels$Class, k=i)
  predTableTemp <- table(knnTemp, testLabels[[1]])
  accuracy <- round(sum(diag(predTableTemp)) / nrow(testData), 4) * 100
  knnResults[[i]] <- predTableTemp
  knnAccuracies[[i]] <- accuracy
}

# Displaying best model results for k=1:100
knnAccuracies <- unlist(knnAccuracies)
bestAccuracy <- max(knnAccuracies)
bestIndex <- which(knnAccuracies == max(knnAccuracies))[1]
predTable <- knnResults[[bestIndex]]

cat("Best model accuracy was ", bestAccuracy, "% with k=", bestIndex, "\n", sep="")
predTable

# Plotting results for k=1:100
ggplot(data=NULL, aes(x=1:100, y=knnAccuracies)) +
  geom_smooth() +
  theme_bw() +
  labs(title="K-Nearest Neighbors, k=1:100", x="k", y="Accuracy")
ggsave(paste0(projRoot, "figures/knn-accuracies.png"), device="png", width=6, height=4)

# Exporting results table to .csv file
predTable <- as.data.frame.matrix(predTable)
write.csv(predTable, file=paste0(projRoot, "tables/knn.csv"))

```

Write about k value selection

## Support Vector Machines
```{r svm, warning=FALSE}

# Setting chunk seed
set.seed(123)

# Preparing principal component data for use with SVM
svmData <- pca8
svmData$Class <- as.factor(labels$Class)

# Splitting data into training and test sets, 75-25 split
trainIndices <- sample(1:nrow(svmData), 0.75*nrow(svmData), replace=FALSE)

trainData <- svmData[trainIndices,]
testData <- svmData[-trainIndices,]

# Training model and tuning hyperparameters with e1071::tune
tuneRadialSVM <- tune(svm,
                      train.x=trainData[, 1:8],
                      train.y=trainData$Class,
                      kernel="radial",
                      type="C",
                      ranges=list(cost=c(0.01, 0.1, 1, 5, 10, 50),
                                  gamma=c(0.5, 1, 2, 3, 4)))
summary(tuneRadialSVM)

# Extracting best resulting model
bestRadialSVM <- tuneRadialSVM$best.model
summary(bestRadialSVM)

# Testing best model and displaying results
yPred <- predict(bestRadialSVM, testData[,1:8])

predTable <- table(predicted=yPred, truth=testData$Class)
predTable

# Calculating error rate
cat("SVM Accuracy: ", round(sum(diag(predTable)) / nrow(testData), 4) * 100, "%\n", sep="")

# Exporting results table to .csv file
predTable <- as.data.frame.matrix(predTable)
write.csv(predTable, file=paste0(projRoot, "tables/radial-svm.csv"))

```

## Linear Discriminant Analysis
```{r lda}

```

## Quadratic Discriminant Analysis
```{r qda}

```

## Neural Network
```{r nn}

# Setting chunk seed
set.seed(123)

# Preparing principal component data for use with NN
nnData <- pca8
nnData$Class <- labels$Class

# Splitting data into train and test sets, 75-25 split
trainIndices <- sample(1:nrow(nnData), nrow(nnData)*0.75, replace=FALSE)

trainData <- nnData[trainIndices,]
testData <- nnData[-trainIndices,]

# Fitting the model
nnet <- neuralnet(Class ~ ., trainData, hidden=c(10, 12), act.fct="logistic", linear.output=FALSE)

# Assessing model accuracy
yPred <- neuralnet::compute(nnet, testData)
yHat <- yPred$net.result

# Displaying the model
plot(nnet, show.weights=FALSE, information=TRUE, intercept=FALSE, rep="best", col.hidden="blue")

```













